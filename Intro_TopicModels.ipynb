{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro_TopicModels.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/librairy/notebooks/blob/master/Intro_TopicModels.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "AVx3oh5JCi_F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction to Topic Models\n",
        "\n",
        "This Google Colab Notebook serves as an introduction to Probabilistic Topic Models. \n",
        "\n",
        "Textual data can be loaded from a Google Sheet and topics derived from  LDA can be generated. \n",
        "\n",
        "First, it is necessary to indicate the training google sheet and the number of words to show per topic.\n"
      ]
    },
    {
      "metadata": {
        "id": "UcubynMaDbzt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Google Colab Authentication\n",
        "!pip install --upgrade -q gspread\n",
        "#!pip install -q gensim\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eBOg8tF0dBC1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load Input Data\n"
      ]
    },
    {
      "metadata": {
        "id": "VXu9rcR2Dmz7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Load and preview data from a Google Sheet\n",
        "\n",
        "corpus = 'texts' #@param {type:\"string\"}\n",
        "preview = 10 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "worksheet = gc.open(corpus).sheet1\n",
        "\n",
        "# get_all_values gives a list of rows.\n",
        "rows = worksheet.get_all_values()\n",
        "\n",
        "# convert the 3rd column values to a list\n",
        "documents = []\n",
        "for row in rows[1:]:\n",
        "  documents.append(row[2])\n",
        "            \n",
        "# Convert to a DataFrame and render.\n",
        "import pandas as pd\n",
        "dataset_df = pd.DataFrame.from_records(rows)\n",
        "dataset_df.head(n=preview)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ChKtbgQldfTf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# PreProcess Texts\n",
        "\n",
        "Create bag-of-words from tokens in Document:\n"
      ]
    },
    {
      "metadata": {
        "id": "BwiEHyUEdhOl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Tokenization\n",
        "\n",
        "tf_vectorizer = CountVectorizer(\n",
        "    stop_words=None,\n",
        "    min_df=1,\n",
        "    max_df=0.95,\n",
        "    lowercase=False,\n",
        "    max_features=None,\n",
        "    ngram_range=(1,1),\n",
        "    analyzer = 'word'\n",
        ")\n",
        "tf = tf_vectorizer.fit_transform(documents)\n",
        "tf_feature_names = tf_vectorizer.get_feature_names()\n",
        "vocab = tf_vectorizer.vocabulary_\n",
        "\n",
        "print(\"Vocabulary Size: \", len(tf_feature_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lmTsfABkDzF2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Build a Topic Model\n",
        "\n",
        "Now it's time to build a topic model by setting values for:\n",
        "- number of topics\n",
        "- alpha\n",
        "- beta"
      ]
    },
    {
      "metadata": {
        "id": "BP5oPuB9GPfv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Run LDA\n",
        "\n",
        "topics = 2 #@param {type:\"integer\"}\n",
        "\n",
        "alpha = 1.0 #@param {type:\"number\"}\n",
        "\n",
        "beta = 1.0 #@param {type:\"number\"}\n",
        "\n",
        "no_top_words = 10\n",
        "\n",
        "no_top_documents = 3\n",
        "\n",
        "\n",
        "# Run LDA\n",
        "lda_model = LatentDirichletAllocation(\n",
        "    n_components=topics, \n",
        "    doc_topic_prior=alpha, \n",
        "    topic_word_prior=beta, \n",
        "    max_iter=100, \n",
        "    learning_method='online', \n",
        "    learning_offset=50.,\n",
        "    random_state=0).fit(tf)\n",
        "lda_W = lda_model.transform(tf)\n",
        "lda_H = lda_model.components_\n",
        "\n",
        "print(\"LDA Topics\")\n",
        "for topic_idx, topic in enumerate(lda_H):\n",
        "    print(\"-\"*30)\n",
        "    print(\" Topic \",(topic_idx),\" :\")\n",
        "    print(\"[\",\" | \".join([tf_feature_names[i]\n",
        "                    for i in topic.argsort()[:-no_top_words - 1:-1]]),\"]\")\n",
        "    top_doc_indices = np.argsort( lda_W[:,topic_idx] )[::-1][0:no_top_documents]\n",
        "    for doc_index in top_doc_indices:\n",
        "        row_index = doc_index +1\n",
        "        print(\"[\",doc_index,\"] (\",rows[row_index][0],\") \\'\",rows[row_index][1],\"\\'\")\n",
        "        print(\"\\t\",lda_W[doc_index])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xqS0dAIHazjQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sorted List of Terms by  Frequency"
      ]
    },
    {
      "metadata": {
        "id": "p3rsRA3Da01c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "s = tf.toarray().sum(axis=0)\n",
        "st = sorted(range(len(s)), key=lambda k: s[k], reverse=True)\n",
        "for i,x in enumerate(st[:20]):\n",
        "  print(tf_vectorizer.get_feature_names()[x],s[x])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "80GMbeBiV9T7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Get Topic Distributions for each training document\n"
      ]
    },
    {
      "metadata": {
        "id": "_BFnUvDbWBGU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i,v in enumerate(lda_W):\n",
        "  print(\"(\",i,\")\",rows[i+1][0],\":\",v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vtzhJ6BZNQ2z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Infer Topic Distribution for a new Document"
      ]
    },
    {
      "metadata": {
        "id": "S4zVOinlMBRR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text = \"this is an example\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"Topic Distribution: \", lda_model.transform(tf_vectorizer.transform([text])))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}