{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro_TopicModels.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/librairy/notebooks/blob/master/Intro_TopicModels.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "AVx3oh5JCi_F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This Google Colab Notebook serves as an introduction to Probabilistic Topic Models. Textual data can be loaded from a Google Sheet and topics derived from  LDA can be generated. \n",
        "\n",
        "First, it is necessary to indicate the training google sheet and the number of words to show per topic.\n"
      ]
    },
    {
      "metadata": {
        "id": "UcubynMaDbzt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "73acaef9-ed45-469e-8f04-e593108eec8f"
      },
      "cell_type": "code",
      "source": [
        "#@title Google Colab Authentication\n",
        "!pip install --upgrade -q gspread\n",
        "#!pip install -q gensim\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
        "    for topic_idx, topic in enumerate(H):\n",
        "        print(\"-\"*30)\n",
        "        print(\" Topic \",(topic_idx),\" :\")\n",
        "        print(\"[\",\" | \".join([feature_names[i]\n",
        "                        for i in topic.argsort()[:-no_top_words - 1:-1]]),\"]\")\n",
        "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
        "        for doc_index in top_doc_indices:\n",
        "            print(\"(\",doc_index,\")\",documents[doc_index], W[doc_index])\n",
        "\n",
        "\n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-10-15 11:06:08,166 : WARNING : No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "VXu9rcR2Dmz7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "10f2b3b4-656b-4481-c3b4-b74bf01dc8e9"
      },
      "cell_type": "code",
      "source": [
        "#@title Load and preview data from a Google Sheet\n",
        "\n",
        "googlesheet_filename = 'sample' #@param {type:\"string\"}\n",
        "data_rows_to_preview = 10 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "worksheet = gc.open(googlesheet_filename).sheet1\n",
        "\n",
        "# get_all_values gives a list of rows.\n",
        "rows = worksheet.get_all_values()\n",
        "\n",
        "# convert the 2nd column values to a list\n",
        "documents = []\n",
        "for row in rows[1:]:\n",
        "  documents.append(row[1])\n",
        "  \n",
        "#print(documents)\n",
        "\n",
        "# Convert to a DataFrame and render.\n",
        "import pandas as pd\n",
        "dataset_df = pd.DataFrame.from_records(rows)\n",
        "dataset_df.head(n=data_rows_to_preview)\n",
        "\n",
        "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
        "tf_vectorizer = CountVectorizer(\n",
        "    stop_words='english',\n",
        "    min_df=2,\n",
        "    max_df=0.95,\n",
        "    lowercase=True,\n",
        "    ngram_range=(1,1)\n",
        ")\n",
        "tf = tf_vectorizer.fit_transform(documents)\n",
        "tf_feature_names = tf_vectorizer.get_feature_names()\n",
        "vocab = tf_vectorizer.vocabulary_\n",
        "\n",
        "print(\"Vocabulary Size: \", len(tf_feature_names))"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-10-15 11:08:41,064 : INFO : Refreshing access_token\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size:  10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lmTsfABkDzF2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now it's time to build a topic model by setting values for:\n",
        "- number of topics\n",
        "- alpha\n",
        "- beta"
      ]
    },
    {
      "metadata": {
        "id": "BP5oPuB9GPfv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "outputId": "7f2f4b84-66c7-4520-d311-9ad593a76c21"
      },
      "cell_type": "code",
      "source": [
        "#@title Run LDA\n",
        "\n",
        "topics = 4 #@param {type:\"integer\"}\n",
        "\n",
        "alpha = 0.1 #@param {type:\"number\"}\n",
        "\n",
        "beta = 0.1 #@param {type:\"number\"}\n",
        "\n",
        "no_top_words = 5\n",
        "\n",
        "no_top_documents = 3\n",
        "\n",
        "\n",
        "# Run LDA\n",
        "lda_model = LatentDirichletAllocation(\n",
        "    n_components=topics, \n",
        "    doc_topic_prior=alpha, \n",
        "    topic_word_prior=beta, \n",
        "    max_iter=20, \n",
        "    learning_method='online', \n",
        "    learning_offset=50.,\n",
        "    random_state=0).fit(tf)\n",
        "lda_W = lda_model.transform(tf)\n",
        "lda_H = lda_model.components_\n",
        "\n",
        "print(\"LDA Topics\")\n",
        "display_topics(lda_H, lda_W, tf_feature_names, documents, no_top_words, no_top_documents)\n"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LDA Topics\n",
            "------------------------------\n",
            " Topic  0  :\n",
            "[ graph | trees | minors | computer | response ]\n",
            "( 7 ) Graph minors: Widths of trees and quasi-ordering [0.91176294 0.02941196 0.02941266 0.02941244]\n",
            "( 8 ) Graph minors: a suervey in data from graphical point of view [0.87499749 0.04166696 0.04166814 0.04166742]\n",
            "( 6 ) the intersection graph of paths in trees [0.87499729 0.04166692 0.04166786 0.04166793]\n",
            "------------------------------\n",
            " Topic  1  :\n",
            "[ response | computer | time | human | user ]\n",
            "( 1 ) A survey of user opinion of computer system response time [0.02272786 0.9318136  0.02272864 0.02272991]\n",
            "( 4 ) Relation of user-perceived response time to error measurement [0.02941249 0.91175796 0.02941341 0.02941613]\n",
            "( 0 ) human machine interface for Lab ABC computer applications [0.02941265 0.9117546  0.02941397 0.02941878]\n",
            "------------------------------\n",
            " Topic  2  :\n",
            "[ computer | eps | user | time | minors ]\n",
            "( 5 ) The generation of random, binary, unordered trees [0.78570752 0.07142924 0.07143106 0.07143217]\n",
            "( 3 ) System and human system engineering testing of EPS [0.0416682  0.87497362 0.04167149 0.04168668]\n",
            "( 8 ) Graph minors: a suervey in data from graphical point of view [0.87499749 0.04166696 0.04166814 0.04166742]\n",
            "------------------------------\n",
            " Topic  3  :\n",
            "[ eps | user | interface | trees | time ]\n",
            "( 2 ) The EPS user interface management system [0.02941243 0.02941805 0.02941369 0.91175584]\n",
            "( 5 ) The generation of random, binary, unordered trees [0.78570752 0.07142924 0.07143106 0.07143217]\n",
            "( 3 ) System and human system engineering testing of EPS [0.0416682  0.87497362 0.04167149 0.04168668]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "80GMbeBiV9T7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Get Topic Distributions:\n"
      ]
    },
    {
      "metadata": {
        "id": "_BFnUvDbWBGU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "014d7b8d-6b8f-4f69-de7c-9b0d67d52894"
      },
      "cell_type": "code",
      "source": [
        "#@title Topic Distributions\n",
        "\n",
        "\n",
        "print(\"Topic Distributions: \")\n",
        "\n",
        "bounds = (0,5)\n",
        "tds = lda_model.transform(tf[bounds[0]:bounds[1]])\n",
        "for x in range(bounds[0],bounds[1]):\n",
        "  print(\"Doc\",x,tds[x])\n"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic Distributions: \n",
            "Doc 0 [0.02941265 0.9117546  0.02941397 0.02941878]\n",
            "Doc 1 [0.02272786 0.9318136  0.02272864 0.02272991]\n",
            "Doc 2 [0.02941243 0.02941805 0.02941369 0.91175584]\n",
            "Doc 3 [0.0416682  0.87497362 0.04167149 0.04168668]\n",
            "Doc 4 [0.02941249 0.91175796 0.02941341 0.02941613]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vtzhJ6BZNQ2z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Get topics for a given  sample:"
      ]
    },
    {
      "metadata": {
        "id": "S4zVOinlMBRR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "718cb0eb-c33a-4ea6-bec2-019eb1e10f54"
      },
      "cell_type": "code",
      "source": [
        "#@title Inference\n",
        "\n",
        "text = \"this is an example\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"Topic Distribution: \", lda_model.transform(tf_vectorizer.transform([text])))\n"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic Distribution:  [[0.25 0.25 0.25 0.25]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}